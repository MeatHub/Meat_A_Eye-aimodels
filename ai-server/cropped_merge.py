import os
from pathlib import Path
# Hugging Face / transformers ë‚´ë¶€ ì§„í–‰ ë°”ë¡œ ì¸í•´ 'í”„ë¡œì„¸ì‹±'ì´ ë‘ ë²ˆ ë³´ì´ëŠ” ê²ƒ ë°©ì§€
os.environ.setdefault("HF_HUB_DISABLE_PROGRESS_BARS", "1")
os.environ.setdefault("TRANSFORMERS_VERBOSITY", "error")

import warnings
import logging
import torch
import shutil
import random
import numpy as np
from PIL import Image
from transformers import pipeline
import easyocr
from tqdm import tqdm

# MPS(Mac)ì—ì„œëŠ” pin_memory ë¯¸ì§€ì› â†’ PyTorch DataLoader ê²½ê³  ë¬´ì‹œ (ë™ì‘ì—ëŠ” ì˜í–¥ ì—†ìŒ)
warnings.filterwarnings("ignore", message=".*pin_memory.*MPS.*")
# Hugging Face pipeline ë‚´ë¶€ ì§„í–‰ ë¡œê·¸ ë¹„í™œì„±í™” â†’ tqdm í•˜ë‚˜ë§Œ ë³´ì´ë„ë¡
logging.getLogger("transformers.pipelines.base").setLevel(logging.WARNING)

# ==========================================
# 1. ê²½ë¡œ ì„¤ì • (íŒ€ì¥ë‹˜ í™˜ê²½)
# ==========================================
# [ì¤‘ìš”] ë¶€ìœ„ ë°”ê¿€ ë•Œ ì•„ë˜ ë‘ ê°œë¥¼ ë°˜ë“œì‹œ ê°™ì€ ë¶€ìœ„ë¡œ ë§ì¶œ ê²ƒ!
#       ì…ë ¥ í´ë” = raw_images/{ë¶€ìœ„}, PREFIX = master_dataset ì €ì¥ í´ë”ëª…
BASE = Path(__file__).resolve().parent.parent / "data"
RAW_INPUT_FOLDER = BASE / "raw_images" / "Pork_Loin"   # ì •ì œí•  ì›ë³¸ ì´ë¯¸ì§€ í´ë” (ë¶€ìœ„ë³„ë¡œ ë³€ê²½)
MASTER_DATA_ROOT = BASE / "master_dataset"
FINAL_SPLIT_ROOT = BASE / "dataset_final"

PREFIX = "Pork_Loin"   # master_dataset ì•ˆì— ë§Œë“¤ì–´ì§ˆ í´ë”ëª… (RAW_INPUT_FOLDER ë¶€ìœ„ì™€ ë™ì¼í•˜ê²Œ)
# ëª©í‘œ ë¹„ìœ¨ (8:1:1)
RATIOS = {'train': 0.8, 'val': 0.1, 'test': 0.1}
# True: ì •ì œ í›„ dataset_finalë¡œ ì´ë™, False: master_datasetì—ë§Œ ì €ì¥ (ì••ì¶• í›„ êµ¬ê¸€ ë“œë¼ì´ë¸Œ ì—…ë¡œë“œìš©)
SEND_TO_DATASET_FINAL = False

# í•„í„°ë§ ì„¤ì •
THRESHOLD = 0.35
MIN_SIZE = 640
OCR_CONFIDENCE = 0.4

os.makedirs(os.path.join(MASTER_DATA_ROOT, PREFIX), exist_ok=True)

# ==========================================
# 2. ëª¨ë¸ ë¡œë“œ (Mac M2: MPS ì‹œë„ â†’ ì‹¤íŒ¨ ì‹œ CPU, NVIDIA: CUDA)
# ==========================================
if getattr(torch.backends, "mps", None) and torch.backends.mps.is_available():
    _device = "mps"
elif torch.cuda.is_available():
    _device = 0
else:
    _device = -1
try:
    detector = pipeline(model="IDEA-Research/grounding-dino-base", task="zero-shot-object-detection", device=_device)
    print(f"[cropped_merge] ì¶”ë¡  ë””ë°”ì´ìŠ¤: {_device}")
except Exception as e:
    print(f"[cropped_merge] MPS/CUDA ë¡œë“œ ì‹¤íŒ¨, CPU ì‚¬ìš©: {e}")
    detector = pipeline(model="IDEA-Research/grounding-dino-base", task="zero-shot-object-detection", device=-1)
# EasyOCRëŠ” CUDAë§Œ ì§€ì›í•˜ë¯€ë¡œ Macì—ì„œëŠ” CPU ì‚¬ìš©
reader = easyocr.Reader(['ko', 'en'], gpu=torch.cuda.is_available())

# ==========================================
# 3. ìŠ¤ë§ˆíŠ¸ ë™ê¸°í™” í•¨ìˆ˜ë“¤
# ==========================================

def get_current_split_files():
    """í˜„ì¬ Train/Val/Test í´ë”ì— ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜µë‹ˆë‹¤."""
    existing_files = {'train': set(), 'val': set(), 'test': set(), 'all': set()}
    for split in ['train', 'val', 'test']:
        path = os.path.join(FINAL_SPLIT_ROOT, split, PREFIX)
        if os.path.exists(path):
            files = [f for f in os.listdir(path) if f.endswith('.jpg')]
            existing_files[split] = set(files)
            existing_files['all'].update(files)
    return existing_files

def get_next_vacant_number(master_dir, prefix):
    """ë§ˆìŠ¤í„°ì™€ ìŠ¤í”Œë¦¿ í´ë” ëª¨ë‘ë¥¼ ë’¤ì ¸ì„œ ë¹„ì–´ìˆëŠ” ê°€ì¥ ë¹ ë¥¸ ë²ˆí˜¸ë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
    split_info = get_current_split_files()
    i = 1
    while True:
        filename = f"{prefix}_{i:04d}.jpg"
        master_path = os.path.join(master_dir, filename)
        # ë§ˆìŠ¤í„°ì—ë„ ì—†ê³ , ì–´ë–¤ ìŠ¤í”Œë¦¿ í´ë”ì—ë„ ì—†ëŠ” ë²ˆí˜¸ê°€ 'ì§„ì§œ ë¹ˆìë¦¬'
        if not os.path.exists(master_path) and filename not in split_info['all']:
            return i, master_path
        i += 1


def average_hash(image: Image.Image, hash_size: int = 16) -> np.ndarray:
    """
    ê°„ë‹¨í•œ aHash(average hash) êµ¬í˜„.
    - ì´ë¯¸ì§€ë¥¼ ì‘ì€ ê·¸ë ˆì´ìŠ¤ì¼€ì¼ë¡œ ë¦¬ì‚¬ì´ì¦ˆ í›„
    - í”½ì…€ í‰ê· ë³´ë‹¤ í¬ë©´ 1, ì•„ë‹ˆë©´ 0
    """
    img = image.convert("L").resize((hash_size, hash_size), Image.BILINEAR)
    pixels = np.asarray(img, dtype=np.float32)
    mean = pixels.mean()
    return (pixels > mean).astype(np.uint8).flatten()


def hamming_distance(a: np.ndarray, b: np.ndarray) -> int:
    """ë‘ í•´ì‹œ(0/1 ë°°ì—´) ì‚¬ì´ì˜ í•´ë° ê±°ë¦¬."""
    # ê¸¸ì´ê°€ ë‹¤ë¥¸ ê²½ìš°ë¥¼ ë°©ì§€
    if a.shape != b.shape:
        return 9999
    return int(np.count_nonzero(a != b))


def remove_near_duplicate_images(folder: str, hamming_thresh: int = 5) -> int:
    """
    master_dataset/{PREFIX} ì•ˆì—ì„œ 'ì—°íƒ€ë¡œ ì°íŒ ê±°ì˜ ê°™ì€ ì‚¬ì§„'ì„ ì •ë¦¬.
    - íŒŒì¼ëª…ì„ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬ í›„, ì´ì›ƒí•œ ì´ë¯¸ì§€ë¼ë¦¬ë§Œ ë¹„êµ (ì—°ì‚¬ ê¸°ì¤€)
    - average hashì˜ í•´ë° ê±°ë¦¬ê°€ hamming_thresh ì´í•˜ì´ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼í•˜ê³  ë’¤ì— ê²ƒì„ ì‚­ì œ

    Returns:
        ì‚­ì œëœ ì´ë¯¸ì§€ ê°œìˆ˜
    """
    if not os.path.exists(folder):
        return 0

    files = [f for f in os.listdir(folder) if f.lower().endswith(".jpg")]
    files.sort()

    if len(files) < 2:
        return 0

    prev_hash = None
    prev_name = None
    deleted = 0

    print(f"\nğŸ§¹ ì¤‘ë³µ/ì—°ì‚¬ ì´ë¯¸ì§€ ì •ë¦¬ ì‹œì‘ ({len(files)}ì¥ ëŒ€ìƒ, ê¸°ì¤€={hamming_thresh})")

    for name in files:
        path = os.path.join(folder, name)
        try:
            img = Image.open(path).convert("RGB")
        except Exception as e:
            print(f"   âš ï¸ {name} ì—´ê¸° ì‹¤íŒ¨, ê±´ë„ˆëœ€: {e}")
            continue

        curr_hash = average_hash(img)

        if prev_hash is not None:
            dist = hamming_distance(prev_hash, curr_hash)
            # í•´ë° ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ë” ë¹„ìŠ·í•œ ì´ë¯¸ì§€
            if dist <= hamming_thresh:
                try:
                    os.remove(path)
                    deleted += 1
                    print(f"   âŒ ì¤‘ë³µ ì‚­ì œ: {name} (ê¸°ì¤€: {prev_name}, ê±°ë¦¬={dist})")
                    continue  # prev_hash ìœ ì§€ (ê°€ì¥ ì•ì˜ ê²ƒë§Œ ë‚¨ê¹€)
                except Exception as e:
                    print(f"   âš ï¸ {name} ì‚­ì œ ì‹¤íŒ¨: {e}")
                    # ì‚­ì œ ì‹¤íŒ¨ ì‹œì—ëŠ” í•´ì‹œë¥¼ ê°±ì‹ í•´ ì¤‘ë³µ ì—°ì‡„ë¥¼ ë§‰ìŒ

        prev_hash = curr_hash
        prev_name = name

    print(f"ğŸ§¹ ì¤‘ë³µ/ì—°ì‚¬ ì •ë¦¬ ì™„ë£Œ: {deleted}ì¥ ì‚­ì œ")
    return deleted

# ==========================================
# 4. í†µí•© ì‹¤í–‰ ë¡œì§
# ==========================================
def run_smart_sync_pipeline():
    # --- STEP 1: ë§ˆìŠ¤í„° ë™ê¸°í™” ë° í¬ë¡­ ---
    print(f"ğŸš€ [Step 1] {PREFIX} ì •ì œ ë° ë¹ˆìë¦¬ ì±„ìš°ê¸° ì‹œì‘...")
    # Path ê°ì²´ë¥¼ strë¡œ ë³€í™˜í•˜ì—¬ os.listdir ì‚¬ìš©
    raw_folder_str = str(RAW_INPUT_FOLDER)
    image_files = [f for f in os.listdir(raw_folder_str) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]
    print(f"ğŸ“ ì›ë³¸ ì´ë¯¸ì§€ íŒŒì¼: {len(image_files)}ê°œ ë°œê²¬")
    target_master_dir = os.path.join(MASTER_DATA_ROOT, PREFIX)
    
    # í†µê³„ ì¶”ì 
    stats = {
        'total_images': len(image_files),
        'no_detection': 0,  # Grounding DINOì—ì„œ ì•„ë¬´ê²ƒë„ ëª» ì°¾ìŒ
        'too_small': 0,     # í¬ê¸°ê°€ MIN_SIZE ë¯¸ë§Œ
        'has_text': 0,      # OCRë¡œ í…ìŠ¤íŠ¸ ê°ì§€ë¨
        'saved': 0          # ìµœì¢… ì €ì¥ë¨
    }
    
    new_crops = []
    for filename in tqdm(image_files, desc="ì´ë¯¸ì§€ ì •ì œ"):
        img_path = os.path.join(raw_folder_str, filename)
        try:
            image = Image.open(img_path).convert("RGB")
            results = detector(image, candidate_labels=["raw pork meat"], threshold=THRESHOLD)
            
            if not results:
                stats['no_detection'] += 1
                continue
            
            found_valid_crop = False
            for res in results:
                box = res['box']
                l, t, r, b = int(box['xmin']), int(box['ymin']), int(box['xmax']), int(box['ymax'])
                width = r - l
                height = b - t
                
                if width < MIN_SIZE or height < MIN_SIZE:
                    stats['too_small'] += 1
                    continue
                
                cropped_img = image.crop((l, t, r, b))
                ocr_results = reader.readtext(np.array(cropped_img))
                if any(prob > OCR_CONFIDENCE for (_, _, prob) in ocr_results):
                    stats['has_text'] += 1
                    continue

                # ë¹ˆ ë²ˆí˜¸ ì°¾ì•„ì„œ ë§ˆìŠ¤í„°ì— ì„ì‹œ ì €ì¥
                _, save_path = get_next_vacant_number(target_master_dir, PREFIX)
                cropped_img.save(save_path, quality=100)
                new_crops.append(os.path.basename(save_path))
                stats['saved'] += 1
                found_valid_crop = True
            
        except Exception as e: 
            print(f"âŒ Error {filename}: {e}")
    
    # í†µê³„ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ì •ì œ í†µê³„:")
    print(f"   ì „ì²´ ì´ë¯¸ì§€: {stats['total_images']}ê°œ")
    print(f"   âŒ íƒì§€ ì‹¤íŒ¨ (Grounding DINO): {stats['no_detection']}ê°œ")
    print(f"   âŒ í¬ê¸° ë¶€ì¡± (<{MIN_SIZE}px): {stats['too_small']}ê°œ")
    print(f"   âŒ í…ìŠ¤íŠ¸ ê°ì§€ (OCR >{OCR_CONFIDENCE}): {stats['has_text']}ê°œ")
    print(f"   âœ… ìµœì¢… ì €ì¥: {stats['saved']}ê°œ")
    print(f"{'='*60}")

    # --- STEP 1.5: ì •ì œ ê²°ê³¼ì—ì„œ ì—°ì‚¬/ì¤‘ë³µ ì´ë¯¸ì§€ ì œê±° ---
    # master_dataset/{PREFIX} ì „ì²´ë¥¼ ëŒ€ìƒìœ¼ë¡œ ì¸ì ‘í•œ ì´ë¯¸ì§€ë¼ë¦¬ aHash ë¹„êµ
    removed = remove_near_duplicate_images(str(target_master_dir), hamming_thresh=5)
    if removed > 0:
        print(f"   â• ì¤‘ë³µ/ì—°ì‚¬ ì œê±° í›„ ë‚¨ì€ ì´ë¯¸ì§€ ìˆ˜: {stats['saved']}ê°œ - {removed}ê°œ (ì‚­ì œ) = {stats['saved'] - removed}ê°œ ì˜ˆìƒ")

    # --- STEP 2: ì‹ ê·œ íŒŒì¼ì„ dataset_finalë¡œ ë°°ë¶„ (SEND_TO_DATASET_FINAL=Trueì¼ ë•Œë§Œ) ---
    if SEND_TO_DATASET_FINAL:
        print(f"\nğŸ“‚ [Step 2] ì‹ ê·œ ë°ì´í„°({len(new_crops)}ê°œ) dataset_final ë°°ë¶„ ì‹œì‘...")
        split_info = get_current_split_files()
        for filename in new_crops:
            master_path = os.path.join(target_master_dir, filename)
            current_counts = {k: len(split_info[k]) for k in ['train', 'val', 'test']}
            total = sum(current_counts.values()) + 1
            best_split = 'train'
            max_diff = -1
            for s in ['train', 'val', 'test']:
                diff = RATIOS[s] - (current_counts[s] / total)
                if diff > max_diff:
                    max_diff = diff
                    best_split = s
            target_path = os.path.join(FINAL_SPLIT_ROOT, best_split, PREFIX)
            os.makedirs(target_path, exist_ok=True)
            shutil.move(master_path, os.path.join(target_path, filename))
            split_info[best_split].add(filename)
        print(f"\nâœ¨ ì‘ì—… ì™„ë£Œ! ì‹ ê·œ ë°ì´í„°ê°€ dataset_final(train/val/test)ë¡œ ë°°ë¶„ë˜ì—ˆìŠµë‹ˆë‹¤.")
    else:
        print(f"\nâœ¨ [Step 1ë§Œ ì™„ë£Œ] ì •ì œëœ ë°ì´í„° {len(new_crops)}ê°œê°€ master_dataset/{PREFIX}/ ì—ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"   â†’ ì••ì¶• í›„ êµ¬ê¸€ ë“œë¼ì´ë¸Œì— ì˜¬ë¦¬ë ¤ë©´ master_dataset í´ë”ë¥¼ zip í•˜ì„¸ìš”.")
        print(f"   â†’ ë‚˜ì¤‘ì— dataset_finalë¡œ ë°°ë¶„í•˜ë ¤ë©´ SEND_TO_DATASET_FINAL=True ë¡œ ë°”ê¾¼ ë’¤ split.py ì‹¤í–‰.")

if __name__ == "__main__":
    run_smart_sync_pipeline()