import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, models
from pathlib import Path
import albumentations as A
from albumentations.pytorch import ToTensorV2
import numpy as np
import random

# 1. ì¥ë¹„ ì„¤ì • (ë§¥ë¶ MPS ìš°ì„ , ì—†ìœ¼ë©´ CPU)
DEVICE = torch.device("mps" if torch.backends.mps.is_available() else "cpu")
if torch.cuda.is_available():
    DEVICE = torch.device("cuda")

print(f"ğŸš€ í˜„ì¬ ì‚¬ìš© ì¤‘ì¸ ì¥ë¹„: {DEVICE}")

# ===== ì„¤ì • (ë§¥ë¶ ì—ì–´ ìµœì í™” + íŒŒì¸íŠœë‹) =====
CONFIG = {
    "dataset_root": Path(__file__).resolve().parent.parent / "data" / "dataset_final",
    "model_save_path": Path(__file__).resolve().parent / "models" / "meat_vision_b2_pro.pth",
    "pretrained_model_path": Path(__file__).resolve().parent / "models" / "meat_vision_b2_pro.pth",  # íŒŒì¸íŠœë‹ìš© ê¸°ì¡´ ëª¨ë¸ ê²½ë¡œ
    "num_epochs": 5,              # íŒŒì¸íŠœë‹ìš©ìœ¼ë¡œ ì¤„ì„ (3~5 epoch ê¶Œì¥)
    "batch_size": 16,             # ë§¥ë¶ ì—ì–´ ê¶Œì¥ (ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ 8ë¡œ ì¤„ì´ì„¸ìš”)
    "learning_rate": 5e-6,         # íŒŒì¸íŠœë‹ìš© ë‚®ì€ í•™ìŠµë¥  (Backbone)
    "head_learning_rate": 5e-4,   # íŒŒì¸íŠœë‹ìš© ë‚®ì€ í•™ìŠµë¥  (Classifier)
    "train_ratio": 0.8,
    "image_size": 260,
    "num_workers": 0,             # [ì¤‘ìš”] ë§¥ë¶ ì—ì–´ 8GBì—ì„œëŠ” 0ì´ ê°€ì¥ ì•ˆì „í•©ë‹ˆë‹¤.
    "mixup_alpha": 0.2,
    "fine_tune": True,            # True: ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ í›„ íŒŒì¸íŠœë‹, False: ì²˜ìŒë¶€í„° í•™ìŠµ
    "class_weight_ribeye_tenderloin": 1.3,  # ë“±ì‹¬Â·ì•ˆì‹¬ Loss ê°€ì¤‘ì¹˜ (1.0 = ë¯¸ì ìš©, 1.2~1.5 ê¶Œì¥)
}

# ë””ë ‰í† ë¦¬ ìë™ ìƒì„±
os.makedirs(CONFIG["model_save_path"].parent, exist_ok=True)

# ===== [í•µì‹¬ 1] ì¦ê°• ì „ëµ =====
train_transform = A.Compose([
    A.Resize(CONFIG["image_size"], CONFIG["image_size"]),
    A.Affine(translate_percent=0.1, scale=(0.8, 1.2), rotate=(-30, 30), p=0.5),
    A.HorizontalFlip(p=0.5),
    A.VerticalFlip(p=0.3),
    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.5),
    A.HueSaturationValue(hue_shift_limit=10, sat_shift_limit=20, val_shift_limit=10, p=0.5),
    A.CLAHE(clip_limit=2.0, p=0.3),
    A.CoarseDropout(
        num_holes_range=(1, 8), 
        hole_height_range=(0.02, 0.1), 
        hole_width_range=(0.02, 0.1), 
        p=0.5
    ),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

val_transform = A.Compose([
    A.Resize(CONFIG["image_size"], CONFIG["image_size"]),
    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ToTensorV2(),
])

# ===== [í•µì‹¬ 2] Mixup ë° ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ìˆ˜ì • ì™„ë£Œ) =====

# 1. Mixup ë°ì´í„° ìƒì„± í•¨ìˆ˜ (ì¥ë¹„ ì¶©ëŒ í•´ê²°ë¨)
def mixup_data(x, y, alpha=1.0):
    '''Returns mixed inputs, pairs of targets, and lambda'''
    if alpha > 0:
        lam = np.random.beta(alpha, alpha)
    else:
        lam = 1

    batch_size = x.size()[0]
    
    # [ìˆ˜ì •] ê³„ì‚° ì „ì— ë°ì´í„°ë¥¼ DEVICEë¡œ ë¨¼ì € ë³´ëƒ…ë‹ˆë‹¤.
    x = x.to(DEVICE)
    y = y.to(DEVICE)
    
    index = torch.randperm(batch_size).to(DEVICE)

    mixed_x = lam * x + (1 - lam) * x[index, :]
    y_a, y_b = y, y[index]
    return mixed_x, y_a, y_b, lam

# 2. Mixup ì†ì‹¤ í•¨ìˆ˜ (ëˆ„ë½ ë³µêµ¬ë¨)
def mixup_criterion(criterion, pred, y_a, y_b, lam):
    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)

# 3. ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (ëˆ„ë½ ë³µêµ¬ë¨)
class AlbumentationsDataset(torch.utils.data.Dataset):
    def __init__(self, dataset, transform=None):
        self.dataset = dataset
        self.transform = transform

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        image, label = self.dataset[idx]
        image = np.array(image)
        
        if self.transform:
            augmented = self.transform(image=image)
            image = augmented["image"]
            
        return image, label

# ===== [í•µì‹¬ 3] ëª¨ë¸ ìƒì„± =====
def create_model_b2(num_classes: int):
    model = models.efficientnet_b2(weights=models.EfficientNet_B2_Weights.DEFAULT)
    model.classifier = nn.Sequential(
        nn.Dropout(p=0.4, inplace=True),
        nn.Linear(model.classifier[1].in_features, num_classes)
    )
    return model

# ===== ë©”ì¸ í•¨ìˆ˜ =====
def main():
    # ë°ì´í„°ì…‹ ë¡œë“œ
    print(f"ğŸ“ ë°ì´í„° ì½ëŠ” ì¤‘... ê²½ë¡œ: {CONFIG['dataset_root']}")
    
    # í´ë”ê°€ ì‹¤ì œë¡œ ìˆëŠ”ì§€ ì²´í¬
    if not CONFIG["dataset_root"].exists():
        print(f"âŒ ì—ëŸ¬: ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤! ({CONFIG['dataset_root']})")
        return

    train_dataset = datasets.ImageFolder(root=CONFIG["dataset_root"] / "train")
    val_dataset = datasets.ImageFolder(root=CONFIG["dataset_root"] / "val")
    test_dataset = datasets.ImageFolder(root=CONFIG["dataset_root"] / "test")

    num_classes = len(train_dataset.classes)
    print(f"âœ… í´ë˜ìŠ¤ ê°œìˆ˜: {num_classes}ê°œ")

    # ë“±ì‹¬Â·ì•ˆì‹¬ Loss ê°€ì¤‘ì¹˜ (í´ë˜ìŠ¤ ì¸ë±ìŠ¤ëŠ” ImageFolder ì•ŒíŒŒë²³ ìˆœì„œ)
    weight_val = CONFIG.get("class_weight_ribeye_tenderloin", 1.0)
    if weight_val != 1.0:
        class_weights = torch.ones(num_classes, dtype=torch.float32)
        for name in ("Beef_Ribeye", "Beef_Tenderloin"):
            if name in train_dataset.class_to_idx:
                i = train_dataset.class_to_idx[name]
                class_weights[i] = weight_val
        class_weights = class_weights.to(DEVICE)
        criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)
        print(f"   ğŸ“Œ ë“±ì‹¬Â·ì•ˆì‹¬ Loss ê°€ì¤‘ì¹˜: {weight_val}")
    else:
        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

    # DataLoader ì„¤ì •
    train_loader = DataLoader(AlbumentationsDataset(train_dataset, train_transform), 
                              batch_size=CONFIG["batch_size"], shuffle=True, 
                              num_workers=CONFIG["num_workers"], pin_memory=False) # ë§¥ë¶ì€ pin_memory Falseê°€ ì† í¸í•¨
    
    val_loader = DataLoader(AlbumentationsDataset(val_dataset, val_transform), 
                            batch_size=CONFIG["batch_size"], shuffle=False, 
                            num_workers=CONFIG["num_workers"], pin_memory=False)

    test_loader = DataLoader(AlbumentationsDataset(test_dataset, val_transform),
                             batch_size=CONFIG["batch_size"], shuffle=False, 
                             num_workers=CONFIG["num_workers"], pin_memory=False)

    # ëª¨ë¸ ì¤€ë¹„
    model = create_model_b2(num_classes).to(DEVICE)
    
    # === íŒŒì¸íŠœë‹: ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ (D. í•™ìŠµ ì „ëµ) ===
    if CONFIG["fine_tune"] and os.path.exists(CONFIG["pretrained_model_path"]):
        print(f"\nğŸ“¥ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì¤‘: {CONFIG['pretrained_model_path']}")
        try:
            model.load_state_dict(torch.load(CONFIG["pretrained_model_path"], map_location=DEVICE))
            print("âœ… ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ! íŒŒì¸íŠœë‹ ëª¨ë“œë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        except Exception as e:
            print(f"âš ï¸ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("   ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    else:
        if CONFIG["fine_tune"]:
            print("âš ï¸ ê¸°ì¡´ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        else:
            print("ğŸ†• ì²˜ìŒë¶€í„° í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤.")

    # ì˜µí‹°ë§ˆì´ì € & ìŠ¤ì¼€ì¤„ëŸ¬
    optimizer = optim.AdamW([
        {'params': model.features.parameters(), 'lr': CONFIG["learning_rate"]},
        {'params': model.classifier.parameters(), 'lr': CONFIG["head_learning_rate"]}
    ], weight_decay=1e-2)

    # criterionì€ ìœ„ì—ì„œ class_weight ì ìš© ì—¬ë¶€ì— ë”°ë¼ ì´ë¯¸ ì •ì˜ë¨
    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=10, T_mult=1)

    best_val_acc = 0.0
    
    mode_str = "íŒŒì¸íŠœë‹" if (CONFIG["fine_tune"] and os.path.exists(CONFIG["pretrained_model_path"])) else "ì²˜ìŒë¶€í„° í•™ìŠµ"
    print(f"\nğŸ”¥ {mode_str} ì‹œì‘! (MacBook Airê°€ ì¡°ê¸ˆ ëœ¨ê±°ì›Œì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    print(f"   - Backbone í•™ìŠµë¥ : {CONFIG['learning_rate']}")
    print(f"   - Classifier í•™ìŠµë¥ : {CONFIG['head_learning_rate']}")
    print(f"   - Epoch: {CONFIG['num_epochs']}\n")
    
    for epoch in range(CONFIG["num_epochs"]):
        # === Training Phase ===
        model.train()
        train_loss, train_correct = 0, 0
        
        # ì§„í–‰ ìƒí™© ê°„ë‹¨ í‘œì‹œ
        print(f"\n[Epoch {epoch+1}/{CONFIG['num_epochs']}] í•™ìŠµ ì§„í–‰ ì¤‘...", end=" ")
        
        for inputs, labels in train_loader:
            # inputs, labelsëŠ” mixup í•¨ìˆ˜ ì•ˆì—ì„œ to(DEVICE) ì²˜ë¦¬í•˜ë¯€ë¡œ ì—¬ê¸°ì„  íŒ¨ìŠ¤í•´ë„ ë¨
            # í•˜ì§€ë§Œ ì•ˆì „í•˜ê²Œ í•œë²ˆ ë” í•´ë„ ë¬´ë°©í•¨
            
            # Mixup ì ìš© (ë‚´ë¶€ì—ì„œ DEVICEë¡œ ì´ë™)
            inputs, labels_a, labels_b, lam = mixup_data(inputs, labels, CONFIG["mixup_alpha"])
            
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = mixup_criterion(criterion, outputs, labels_a, labels_b, lam)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            
            # ì •í™•ë„ ê³„ì‚° (Mixup ê³ ë ¤)
            pred = outputs.argmax(1)
            train_correct += (lam * (pred == labels_a).float().sum() + 
                             (1 - lam) * (pred == labels_b).float().sum()).item()

        # === Validation Phase ===
        model.eval()
        val_correct = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = model(inputs)
                val_correct += (outputs.argmax(1) == labels).sum().item()

        scheduler.step()
        
        # ì •í™•ë„ ê³„ì‚°
        t_acc = train_correct / len(train_loader.dataset)
        v_acc = val_correct / len(val_loader.dataset)
        avg_train_loss = train_loss / len(train_loader)
        
        # ëª…í™•í•œ ì¶œë ¥ í˜•ì‹
        print(f"\n{'='*60}")
        print(f"Epoch [{epoch+1}/{CONFIG['num_epochs']}] ê²°ê³¼:")
        print(f"  ğŸ“Š Train Accuracy:  {t_acc:.4f} ({train_correct}/{len(train_loader.dataset)})")
        print(f"  ğŸ“Š Train Loss:       {avg_train_loss:.4f}")
        print(f"  ğŸ“Š Val Accuracy:    {v_acc:.4f} ({val_correct}/{len(val_loader.dataset)})")
        print(f"{'='*60}")

        if v_acc > best_val_acc:
            best_val_acc = v_acc
            torch.save(model.state_dict(), CONFIG["model_save_path"])
            print(f"  â­ ìµœê³  Validation Accuracy ê°±ì‹ ! ëª¨ë¸ ì €ì¥ë¨ ({v_acc:.4f})\n")

    # === ìµœì¢… í…ŒìŠ¤íŠ¸ í‰ê°€ ===
    if os.path.exists(CONFIG["model_save_path"]):
        print("\n=== ğŸ† ìµœì¢… í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ í‰ê°€ ===")
        best_model = create_model_b2(num_classes).to(DEVICE)
        best_model.load_state_dict(torch.load(CONFIG["model_save_path"], map_location=DEVICE))
        best_model.eval()

        test_correct = 0
        with torch.no_grad():
            for inputs, labels in test_loader:
                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)
                outputs = best_model(inputs)
                test_correct += (outputs.argmax(1) == labels).sum().item()

        test_acc = test_correct / len(test_loader.dataset)
        print(f"   ğŸ“Š ìµœì¢… Test Accuracy: {test_acc:.4f}")
    else:
        print("âš ï¸ ëª¨ë¸ íŒŒì¼ì´ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main()