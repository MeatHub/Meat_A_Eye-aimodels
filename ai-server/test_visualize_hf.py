import torch
import torch.nn as nn
from transformers import AutoModelForImageClassification
from torchvision import transforms
import cv2
import numpy as np
import os
from pathlib import Path
from PIL import Image

# ==========================================
# 1. ì„¤ì • (Configuration)
# ==========================================
CONFIG = {
    # í…ŒìŠ¤íŠ¸í•  ì´ë¯¸ì§€ê°€ ë‹´ê¸´ í´ë”
    "test_images_dir": Path(r"D:\ahy\Projects\meathub\Meat_A_Eye-aimodels\data\0205dataset"),
    # ë°©ê¸ˆ í•™ìŠµí•œ B2-HF ê°€ì¤‘ì¹˜ ê²½ë¡œ
    "model_path": Path(r"D:\ahy\Projects\meathub\Meat_A_Eye-aimodels\ai-server\models\models_b2\meat_vision_b2_hf.pth"),
    # ê²°ê³¼ê°€ ì €ì¥ë  í´ë”
    "result_save_dir": Path(r"D:\ahy\Projects\meathub\Meat_A_Eye-aimodels\test0205"),
    
    "hf_model_name": "google/efficientnet-b2",
    
    "class_names": [
        'Beef_BottomRound', 'Beef_Brisket', 'Beef_Chuck', 'Beef_Rib', 'Beef_Ribeye',
        'Beef_Round', 'Beef_Shank', 'Beef_Shoulder', 'Beef_Sirloin', 'Beef_Tenderloin'
    ],
    
    "image_size": 260, # B2 í‘œì¤€ í¬ê¸°
    "device": torch.device("cuda" if torch.cuda.is_available() else "cpu")
}

os.makedirs(CONFIG["result_save_dir"], exist_ok=True)

# ==========================================
# 2. Grad-CAM í´ë˜ìŠ¤ (B2-HF ì „ìš©)
# ==========================================
class GradCAM:
    def __init__(self, model, target_layer):
        self.model = model
        self.target_layer = target_layer
        self.gradients = None
        self.output = None
        
        # í›…(Hook) ë“±ë¡
        self.target_layer.register_forward_hook(self.save_output)
        self.target_layer.register_full_backward_hook(self.save_gradient)

    def save_output(self, module, input, output):
        self.output = output

    def save_gradient(self, module, grad_input, grad_output):
        self.gradients = grad_output[0]

    def generate(self, input_tensor, class_idx):
        self.model.zero_grad()
        outputs = self.model(input_tensor).logits
        loss = outputs[0, class_idx]
        loss.backward()

        # ê°€ì¤‘ì¹˜ ê³„ì‚° (GAP)
        weights = torch.mean(self.gradients, dim=(2, 3), keepdim=True)
        # íˆíŠ¸ë§µ ìƒì„±
        cam = torch.sum(weights * self.output, dim=1).squeeze()
        
        # ReLU ì ìš© ë° ì •ê·œí™”
        cam = np.maximum(cam.detach().cpu().numpy(), 0)
        cam = cam / (np.max(cam) + 1e-7)
        return cam

# ==========================================
# 3. ì‹œê°í™” ì‹¤í–‰ (ìµœì¢… ìˆ˜ì • ë²„ì „)
# ==========================================
def run_visualization():
    print(f"ğŸš€ B2-HF ì‹œê°í™” í…ŒìŠ¤íŠ¸ ì‹œì‘ (Device: {CONFIG['device']})")
    
    # 1. ëª¨ë¸ ë¡œë“œ
    model = AutoModelForImageClassification.from_pretrained(
        CONFIG["hf_model_name"], 
        num_labels=len(CONFIG["class_names"]),
        ignore_mismatched_sizes=True
    )
    
    # í•™ìŠµëœ ê°€ì¤‘ì¹˜ ë¡œë“œ
    model.load_state_dict(torch.load(CONFIG["model_path"], map_location=CONFIG["device"]))
    model.to(CONFIG["device"]).eval()

    # ---------------------------------------------------------
    # â˜… [ìˆ˜ì • í¬ì¸íŠ¸] Hugging Face EfficientNetì˜ ìµœì¢… ë ˆì´ì–´ ê²½ë¡œ
    # ---------------------------------------------------------
    # model (EfficientNetForImageClassification)
    #  â””â”€ efficientnet (EfficientNetModel)
    #      â””â”€ encoder (EfficientNetEncoder)
    #          â””â”€ blocks (nn.ModuleList) <- ì´ ì•ˆì˜ ë§ˆì§€ë§‰ ë¸”ë¡ì„ ì„ íƒí•©ë‹ˆë‹¤.
    try:
        target_layer = model.efficientnet.encoder.blocks[-1]
        print("âœ… íƒ€ê²Ÿ ë ˆì´ì–´(encoder.blocks[-1]) ì„¤ì • ì™„ë£Œ!")
    except AttributeError:
        # í˜¹ì‹œë¼ë„ êµ¬ì¡°ê°€ ë‹¤ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•´ í•˜ìœ„ ëª¨ë“ˆì„ ì¶œë ¥í•´ë´…ë‹ˆë‹¤.
        print("âŒ ë ˆì´ì–´ êµ¬ì¡°ê°€ ì˜ˆìƒê³¼ ë‹¤ë¦…ë‹ˆë‹¤. ì•„ë˜ êµ¬ì¡°ë¥¼ í™•ì¸í•˜ì„¸ìš”:")
        print(model)
        return
    # ---------------------------------------------------------
    
    grad_cam = GradCAM(model, target_layer)

    # ... (ì´í›„ ì „ì²˜ë¦¬ ë° ì‹œê°í™” ë¡œì§ì€ ê¸°ì¡´ê³¼ ë™ì¼)

    # ... (ì´í•˜ ì „ì²˜ë¦¬ ë° ë£¨í”„ ë¡œì§ì€ ë™ì¼)

    # 3. ì „ì²˜ë¦¬ (í•™ìŠµ ë•Œì™€ ë™ì¼í•˜ê²Œ)
    transform = transforms.Compose([
        transforms.Resize((CONFIG["image_size"], CONFIG["image_size"])),
        transforms.ToTensor(),
        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])
    ])

    image_paths = [p for p in CONFIG["test_images_dir"].glob("*") if p.suffix.lower() in [".jpg", ".png", ".jpeg"]]

    for img_path in image_paths:
        raw_img = cv2.imread(str(img_path))
        if raw_img is None: continue
        
        img_rgb = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB)
        pil_img = Image.fromarray(img_rgb)
        input_tensor = transform(pil_img).unsqueeze(0).to(CONFIG["device"])
        
        # ì¶”ë¡  (ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™” í•„ìš”)
        with torch.enable_grad():
            logits = model(input_tensor).logits
            probs = torch.nn.functional.softmax(logits, dim=1)
            conf, pred_idx = torch.max(probs, 1)
        
        pred_label = CONFIG["class_names"][pred_idx.item()]
        confidence = conf.item() * 100
        
        # íˆíŠ¸ë§µ ìƒì„±
        heatmap = grad_cam.generate(input_tensor, pred_idx.item())
        
        # ì‹œê°í™” ê°€ê³µ
        view_img = cv2.resize(raw_img, (CONFIG["image_size"], CONFIG["image_size"]))
        heatmap_resize = cv2.resize(heatmap, (CONFIG["image_size"], CONFIG["image_size"]))
        heatmap_uint8 = np.uint8(255 * heatmap_resize)
        heatmap_color = cv2.applyColorMap(heatmap_uint8, cv2.COLORMAP_JET)
        
        blended = cv2.addWeighted(view_img, 0.6, heatmap_color, 0.4, 0)
        
        # ìƒë‹¨ ì—¬ë°± ì¶”ê°€ (í…ìŠ¤íŠ¸ìš©)
        header_h = 100
        result_img = cv2.copyMakeBorder(blended, header_h, 0, 0, 0, cv2.BORDER_CONSTANT, value=(0, 0, 0))
        
        # í…ìŠ¤íŠ¸ ì¶œë ¥
        text_1 = f"B2-HF PRED: {pred_label}"
        text_2 = f"CONF: {confidence:.2f}%"
        cv2.putText(result_img, text_1, (15, 40), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1, cv2.LINE_AA)
        cv2.putText(result_img, text_2, (15, 80), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2, cv2.LINE_AA)

        save_path = CONFIG["result_save_dir"] / f"b2hf_vis_{img_path.name}"
        cv2.imwrite(str(save_path), result_img)
        print(f"âœ… [B2-HF] {img_path.name} -> {pred_label}")

    print(f"\nâœ¨ ì‹œê°í™” ì™„ë£Œ! ê²°ê³¼ í™•ì¸: {CONFIG['result_save_dir']}")

if __name__ == "__main__":
    run_visualization()